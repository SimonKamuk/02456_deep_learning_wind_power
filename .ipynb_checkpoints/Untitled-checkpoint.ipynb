{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "data_loc = '/Users/simon/Documents/DTU/9. semester/deep learning/data/modified data'\n",
    "files = os.listdir(data_loc)\n",
    "df_all = []\n",
    "\n",
    "for file in files:\n",
    "    path = os.path.join(data_loc,file)\n",
    "    name, ext = os.path.splitext(file)\n",
    "    if ext != '.csv':\n",
    "        continue\n",
    "    df = pd.read_csv(path)\n",
    "    df.name = name\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        if col_name != 'Date_Time':\n",
    "            df[col_name]=df[col_name].astype('float64')\n",
    "        else:\n",
    "            df['Date_Time'] = pd.to_datetime(df['Date_Time'])\n",
    "    \n",
    "    df_all.append(df)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden1, num_hidden2, num_output, dropout_rate):\n",
    "        super(Net, self).__init__()  \n",
    "        \n",
    "        model = nn.Sequential(\n",
    "          nn.Conv1d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # input layer\n",
    "        self.W_1 = Parameter(init.kaiming_normal_(torch.Tensor(num_hidden1, num_features)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden1), 0))\n",
    "        # hidden layer 1\n",
    "        self.W_2 = Parameter(init.kaiming_normal_(torch.Tensor(num_hidden2, num_hidden1)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_hidden2), 0))\n",
    "        # hidden layer 2\n",
    "        self.W_3 = Parameter(init.kaiming_normal_(torch.Tensor(num_output, num_hidden2)))\n",
    "        self.b_3 = Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
    "        # define activation function in constructor\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Define batch norm layers\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x, self.W_3, self.b_3)\n",
    "        #Try sigmoid activation at end\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_features, num_l1, num_l2, num_classes, dropout_rate)\n",
    "print(net)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay = 1e-5)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have done this ourselves,\n",
    "# but we should be aware of sklearn and its tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# normalize the inputs\n",
    "x_train.div_(255)\n",
    "x_valid.div_(255)\n",
    "x_test.div_(255)\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = targets_train[slce]\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(targets_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        \n",
    "        output = net(x_valid[slce])\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(targets_valid[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3_7",
   "language": "python",
   "name": "tf_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
