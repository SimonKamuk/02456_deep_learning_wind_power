{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import helper_functions\n",
    "reload(helper_functions)\n",
    "from helper_functions import accuracy_rate, load_data, capacity, get_good_idx, get_slice, get_k_fold_cv_idx, get_x_sequences_rnn, train, get_all_accuracy_rates, allocate_x_batch_rnn, quantile_score, get_competition_preds\n",
    "\n",
    "nn_type = 'rnn'\n",
    "allocate_x_batch = allocate_x_batch_rnn\n",
    "get_x_sequences = get_x_sequences_rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([164958, 16])\n",
      "torch.Size([118382, 1])\n",
      "2016-01-01 20:00:00\n",
      "2017-05-01 00:00:00\n",
      "111927\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2021)\n",
    "\n",
    "case=3\n",
    "x,x_time,y,y_time,time_dif,idx_offset = load_data(case)\n",
    "\n",
    "# Index offset between start and end of training data for one single prediction\n",
    "# i.e. number of quarters of an hour we wish to train on for each sample\n",
    "pred_seq_len = 6*4+1\n",
    "\n",
    "\n",
    "#i=1000\n",
    "#print(target_time[i])\n",
    "#print(x_time[i+idx_offset-pred_seq_len+1:i+idx_offset+1])\n",
    "\n",
    "good_idx = get_good_idx(x,y,idx_offset,pred_seq_len)\n",
    "\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x_time[0])\n",
    "print(y_time[0])\n",
    "print(len(good_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (rnn_layers): LSTM(16, 50, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (l_out): Linear(in_features=1250, out_features=1, bias=True)\n",
      "  (act): ReLU()\n",
      "  (dropout): Dropout(p=0.1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = x.shape[1]\n",
    "num_channels = input_size\n",
    "hidden_size = 50\n",
    "num_hidden = 2\n",
    "out_size = 1\n",
    "drop_p=0.1\n",
    "rnn_type='lstm'\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  \n",
    "        \n",
    "        \n",
    "        if rnn_type.lower()=='lstm':\n",
    "            self.rnn_layers = nn.LSTM(input_size=num_channels,\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  num_layers=num_hidden+1,\n",
    "                                  batch_first=True, # batch_size x sequence_length x input_size\n",
    "                                  bias=True,\n",
    "                                  dropout=drop_p)\n",
    "        elif rnn_type.lower()=='gru':\n",
    "            self.rnn_layers = nn.GRU(input_size=num_channels,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 num_layers=num_hidden+1,\n",
    "                                 batch_first=True, # batch_size x sequence_length x input_size\n",
    "                                 bias=True,\n",
    "                                 dropout=drop_p)\n",
    "        else:\n",
    "            raise(Exception('unknown rnn type'))\n",
    "            \n",
    "        self.l_out = nn.Linear(in_features=pred_seq_len * hidden_size,\n",
    "                               out_features=out_size,\n",
    "                               bias=True)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x, (h, c) = self.lstm(x)\n",
    "        x = self.rnn_layers(x)[0]\n",
    "        x = x.reshape(-1, pred_seq_len * hidden_size)\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(x)  \n",
    "        x = self.l_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/2y2pqy7n6bzgwt7b424n6v6m0000gn/T/ipykernel_3739/1581872282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptim_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgood_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/02456_deep_learning_wind_power/helper_functions.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(nn_type, x, y, Net, optim_params, num_epochs, batch_size, good_idx, k_fold_size, idx_offset, pred_seq_len, loss, case)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# compute gradients given loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslce\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf_3_7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf_3_7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 1000\n",
    "num_epochs = 10\n",
    "k_fold_size = 2\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "#loss = nn.L1Loss()\n",
    "\n",
    "\n",
    "optim_params = {'lr': 3e-3, 'weight_decay': 0}\n",
    "train_loss, valid_loss, net = train(nn_type, x, y, Net, optim_params, num_epochs, batch_size, good_idx, k_fold_size, idx_offset, pred_seq_len, loss, case)\n",
    "valid_loss[0] = np.sqrt(valid_loss[0])\n",
    "\n",
    "#np.savez(outfile, train_loss=train_loss, valid_loss=valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(len(train_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_loss, 'r', epoch, valid_loss[0, :], 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('MSE [normalized units]')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, capacity(case)*np.array(valid_loss[:-1]).T)\n",
    "plt.legend(['rmse','mae','quantile score'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, valid_loss[-1])\n",
    "plt.xlabel('Updates'), plt.ylabel('Accuracy rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = allocate_x_batch(len(good_idx), input_size, pred_seq_len)\n",
    "predictions = net(get_x_sequences(good_idx, x_batch, idx_offset, pred_seq_len, x)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_time[good_idx], capacity(case)*y[good_idx].detach().numpy(), '.', markersize=0.5, label = 'Measured')\n",
    "plt.plot(y_time[good_idx], capacity(case)*predictions,'.', markersize=0.5, label = 'Predicted')\n",
    "lgnd = plt.legend(markerscale=15)    \n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = allocate_x_batch(96, input_size, pred_seq_len)\n",
    "accuracies, times = get_all_accuracy_rates(net, x, y, y_time, x_batch, get_x_sequences, good_idx, idx_offset, pred_seq_len, case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, accuracies)\n",
    "plt.xticks(rotation=30)\n",
    "plt.ylim(0.3,1)\n",
    "plt.show()\n",
    "print('Mean accuracy rate', np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day=0\n",
    "save=False\n",
    "predictions = get_competition_preds(day,case,get_x_sequences,allocate_x_batch,input_size,pred_seq_len,net,save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3_7",
   "language": "python",
   "name": "tf_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
