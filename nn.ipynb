{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement 1\n",
      "Measurement 2\n",
      "Measurement 3\n",
      "NWP 1\n",
      "NWP 2\n",
      "NWP 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "data_loc = '/Users/simon/Documents/DTU/9. semester/deep learning/data'\n",
    "\n",
    "df_all = []\n",
    "\n",
    "files = os.listdir(os.path.join(data_loc,'modified data'))\n",
    "for file in sorted(files):\n",
    "    path = os.path.join(data_loc,'modified data',file)\n",
    "    name, ext = os.path.splitext(file)\n",
    "    if ext != '.csv':\n",
    "        continue\n",
    "    df = pd.read_csv(path)\n",
    "    df.name = name\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        if col_name != 'Date_Time':\n",
    "            df[col_name]=df[col_name].astype('float64')\n",
    "        else:\n",
    "            df['Date_Time'] = pd.to_datetime(df['Date_Time'])\n",
    "    \n",
    "    df_all.append(df)\n",
    "    print(name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "case=1\n",
    "target = torch.Tensor(df_all[case-1]['Park Power [KW]'].values)\n",
    "target_time = df_all[case-1]['Date_Time']\n",
    "x = torch.Tensor(df_all[case+2].iloc[:,1:].values)\n",
    "x_time = df_all[case+2]['Date_Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-18 06:15:00\n",
      "36618   2017-01-17 06:30:00\n",
      "36619   2017-01-17 06:45:00\n",
      "36620   2017-01-17 07:00:00\n",
      "36621   2017-01-17 07:15:00\n",
      "36622   2017-01-17 07:30:00\n",
      "                ...        \n",
      "36709   2017-01-18 05:15:00\n",
      "36710   2017-01-18 05:30:00\n",
      "36711   2017-01-18 05:45:00\n",
      "36712   2017-01-18 06:00:00\n",
      "36713   2017-01-18 06:15:00\n",
      "Name: Date_Time, Length: 96, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "assert x_time.diff().min()==x_time.diff().max()\n",
    "assert target_time.diff().min()==target_time.diff().max()\n",
    "assert x_time.diff().min()==target_time.diff().min()\n",
    "assert target_time.iloc[-1]==x_time.iloc[-1]\n",
    "\n",
    "time_dif = (target_time.iloc[0]-x_time.iloc[0])\n",
    "idx_offset = time_dif.days*24*4+round(time_dif.seconds/(60*15))\n",
    "\n",
    "idx_train_len = 24*4\n",
    "\n",
    "i=1000\n",
    "print(target_time[i])\n",
    "print(x_time[i+idx_offset-idx_train_len+1:i+idx_offset+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size):\n",
    "        super(Net, self).__init__()  \n",
    "        \n",
    "        model = nn.Sequential(\n",
    "            nn.LSTM(input_size=input_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    bias=True),\n",
    "            nn.ReLu(),\n",
    "            x = x.view(-1, hidden_size) # maybe use flatten?\n",
    "            nn.Linear(in_features=hidden_size,\n",
    "                      out_features=out_size,\n",
    "                      bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(input_size, hidden_size, out_size)\n",
    "print(net)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = targets_train[slce]\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(targets_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        \n",
    "        output = net(x_valid[slce])\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_targs += list(targets_valid[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        \n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3_7",
   "language": "python",
   "name": "tf_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
